{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116e6d02-b3ea-4c9a-9f12-40da794be5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import os\n",
    "import zipfile\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from Func_extr import extract_infos\n",
    "\n",
    "##### EXTRACCIÓN DE COPERMICUS\n",
    "\n",
    "def preprocesarDataFrame(df):\n",
    "    df = df[['valid_time', 'latitude', 'longitude', 'tp', 'skt', 'e', 'ro', 'sf',\n",
    "         'swvl1', 'swvl2', 'swvl3', 'swvl4', 'cvh', 'cvl', 'tvh', 'tvl']]\n",
    "    df = df.rename(columns={\n",
    "        'valid_time': 'date',\n",
    "        'tp': 'total_precipitation',\n",
    "        'skt': 'skin_temperature',\n",
    "        'e': 'evaporation',\n",
    "        'ro': 'runoff',\n",
    "        'sf': 'snowfall',\n",
    "        'swvl1': 'soil_water_l1',\n",
    "        'swvl2': 'soil_water_l2',\n",
    "        'swvl3': 'soil_water_l3',\n",
    "        'swvl4': 'soil_water_l4',\n",
    "        'cvh': 'high_vegetation_cover',\n",
    "        'cvl': 'low_vegetation_cover',\n",
    "        'tvh': 'type_high_vegetation',\n",
    "        'tvl': 'type_low_vegetation'\n",
    "    })\n",
    "    # Convertir la columna 'date' a formato de fecha\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    # Realizar las agregaciones\n",
    "    agg_funcs = {\n",
    "        'total_precipitation': 'sum',\n",
    "        'skin_temperature': 'mean',\n",
    "        'evaporation': 'sum',\n",
    "        'runoff': 'sum',\n",
    "        'snowfall': 'sum',\n",
    "        'soil_water_l1': 'sum',\n",
    "        'soil_water_l2': 'sum',\n",
    "        'soil_water_l3': 'sum',\n",
    "        'soil_water_l4': 'sum',\n",
    "        'high_vegetation_cover': 'mean',\n",
    "        'low_vegetation_cover': 'mean',\n",
    "        'type_high_vegetation': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "        'type_low_vegetation': lambda x: x.mode()[0] if not x.mode().empty else np.nan\n",
    "    }\n",
    "    df = df.groupby(['latitude', 'longitude', 'date']).agg(agg_funcs).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def downloadMesCopernicus(days, year, month):\n",
    "    dataset = \"reanalysis-era5-single-levels\"\n",
    "    request = {\n",
    "        'product_type': ['ensemble_mean'],\n",
    "        'variable': [\n",
    "            'total_precipitation', 'skin_temperature', 'evaporation', \n",
    "            'runoff', 'snowfall', 'volumetric_soil_water_layer_1', \n",
    "            'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3', \n",
    "            'volumetric_soil_water_layer_4', 'high_vegetation_cover', \n",
    "            'low_vegetation_cover', 'type_of_high_vegetation', 'type_of_low_vegetation'\n",
    "        ],\n",
    "        'year': year,\n",
    "        'month': month,\n",
    "        'day': days,\n",
    "        'time': ['00:00', '06:00', '12:00', '18:00'],\n",
    "        'format': 'netcdf',\n",
    "        'area': [40.5, -2.2, 38.1, 0.5]  # Cuenca hidrográfica del Júcar +/-.\n",
    "    }\n",
    "    \n",
    "    client = cdsapi.Client(\n",
    "    url=\"https://cds.climate.copernicus.eu/api\",  # URL de producción\n",
    "    key=\"3ca60b29-89a8-4614-88a3-2b4113bfc5f8\"\n",
    ")\n",
    "    \n",
    "    try:\n",
    "        file_name = client.retrieve(dataset, request).download()\n",
    "        return file_name\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Error en la descarga: {e}\")\n",
    "        return None\n",
    "    \n",
    "def procesarZip(zip_file_name):\n",
    "    dataFrames = []\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        # Listar los archivos en el ZIP\n",
    "        archivos = zip_ref.namelist()\n",
    "        for archivo_nc in archivos: \n",
    "            # Extraer el archivo 'instant'\n",
    "            zip_ref.extract(archivo_nc)\n",
    "            print(f\"Archivo '{archivo_nc}' extraído.\")\n",
    "            ds = xr.open_dataset(archivo_nc)\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "            dataFrames.append (df)\n",
    "            ds.close()\n",
    "            os.remove(archivo_nc)\n",
    "    df1 = dataFrames[1]\n",
    "    df1 = df1.drop(['latitude', 'longitude', 'valid_time'], axis = 1)\n",
    "    df = pd.concat([dataFrames[0],df1], axis = 1)\n",
    "    df = preprocesarDataFrame(df)\n",
    "    fechaMin = df['date'].min()\n",
    "    fechaMax = df['date'].max()\n",
    "    print(f'Datos de coeprnicus con fechas de {fechaMin} a {fechaMax} descargadas correctamente')\n",
    "    os.remove(zip_file_name)\n",
    "    return df\n",
    "\n",
    "def extraccionCopernicus (days,year, month):\n",
    "    \n",
    "    zip_file_name = downloadMesCopernicus(days,year, month)\n",
    "    print(f\"Archivo descargado: {zip_file_name}\")\n",
    "    if zip_file_name is None:\n",
    "        return None\n",
    "    else:\n",
    "        df = procesarZip(zip_file_name)\n",
    "    return df\n",
    "\n",
    "def extraerUltimasFechasCopernicus():\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "        c.date_id, d.date\n",
    "        FROM df_copernicus c JOIN df_date d ON  c.date_id = d.date_id;\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    df_date = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    df_date['date'] = pd.to_datetime(df_date['date']).dt.date\n",
    "    ultima_fecha = df_date['date'].max()\n",
    "    return ultima_fecha\n",
    "    \n",
    "def extraerUltimasFechasRios():\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "        c.date_id, d.date\n",
    "        FROM df_rios_canales c JOIN df_date d ON  c.date_id = d.date_id;\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    df_date = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    df_date['date'] = pd.to_datetime(df_date['date']).dt.date\n",
    "    ultima_fecha = df_date['date'].max()\n",
    "    return ultima_fecha    \n",
    "def generar_diferencias_mes_a_mes(fecha1 ,fecha2):\n",
    "    \"\"\"\n",
    "    Genera una lista de días, meses y años entre dos fechas mes a mes,\n",
    "    considerando la fecha inicial desde su día específico y el día final.\n",
    "\n",
    "    Parámetros:\n",
    "    - fecha_actual: Fecha inicial en formato datetime.date.\n",
    "    - fecha_futura: Fecha final en formato datetime.date.\n",
    "\n",
    "    Retorno:\n",
    "    - Lista de tuplas (días, año, mes), donde:\n",
    "      - días: lista de strings con los días del mes ('15', '16', ..., '30')\n",
    "      - año: año correspondiente\n",
    "      - mes: mes correspondiente\n",
    "    \"\"\"\n",
    "    diferencias = []\n",
    "    fecha1 = fecha1  + timedelta(days=1)\n",
    "    fecha_inicio = fecha1  \n",
    "\n",
    "    while fecha_inicio <= fecha2:\n",
    "        # Obtener año y mes actuales\n",
    "        year = fecha_inicio.year\n",
    "        month = fecha_inicio.month\n",
    "\n",
    "        # Si estamos en el primer mes, iniciar desde el día específico de la fecha actual\n",
    "        if fecha_inicio == fecha1:\n",
    "            start_day = fecha_inicio.day\n",
    "        else:\n",
    "            start_day = 1\n",
    "        \n",
    "        # Para el último mes (fecha_futura), restringir al día final\n",
    "        if fecha_inicio.month == fecha2.month:\n",
    "            end_day = fecha2.day\n",
    "        else:\n",
    "            # Si no es el último mes, el mes completo\n",
    "            end_day = monthrange(year, month)[1]\n",
    "\n",
    "        # Calcular los días del mes actual, considerando start_day y end_day\n",
    "        days = [f\"{day:02d}\" for day in range(start_day, end_day + 1)]\n",
    "        \n",
    "        # Agregar a la lista de diferencias\n",
    "        diferencias.append((days, year, month))\n",
    "\n",
    "        # Avanzar al siguiente mes\n",
    "        if month == 12:\n",
    "            fecha_inicio = fecha_inicio.replace(year=year + 1, month=1, day=1)\n",
    "        else:\n",
    "            fecha_inicio = fecha_inicio.replace(month=month + 1, day=1)\n",
    "    \n",
    "    return diferencias\n",
    "\n",
    "def fechasActualizarCopernicus():\n",
    "    fecha1 = extraerUltimasFechasCopernicus()\n",
    "    fecha2 = fecha_actual = datetime.now().date()\n",
    "    fechas_new = generar_diferencias_mes_a_mes(fecha1 ,fecha2)\n",
    "    return fechas_new\n",
    "\n",
    "def actualizarTablaCopernicus():\n",
    "    fechas_new = fechasActualizarCopernicus()\n",
    "    dataFrames = []\n",
    "    for i in fechas_new:\n",
    "        days = i[0]\n",
    "        month = i[2]\n",
    "        year = i[1]\n",
    "        df = extraccionCopernicus (days,year, month)\n",
    "        if df is None:\n",
    "            pass\n",
    "        else:\n",
    "            dataFrames.append(df)\n",
    "    df = pd.concat(dataFrames)\n",
    "    return df\n",
    "\n",
    "##### EXTRACCIÓN DE LA CUENCA DEL JUCAR\n",
    "def extraccionRiosCHJ():\n",
    "    url = \"https://aps.chj.es/down/CSV/F2796_Rios_y_Canales_ROEA.zip\"\n",
    "    fecha_inicial = pd.to_datetime(extraerUltimasFechasRios())\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Verificar si la descarga fue exitosa\n",
    "    \n",
    "    # Paso 2: Cargar el contenido del ZIP en memoria\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    target_file = \"F2796_D2_Serie día.csv\"\n",
    "    if target_file in zip_file.namelist():\n",
    "        with zip_file.open(target_file) as file:\n",
    "            # Leer el archivo CSV directamente como DataFrame\n",
    "            df_rios_canales = pd.read_csv(file, sep=\";\", encoding=\"latin1\")  # Ajusta el separador y la codificación si es necesario\n",
    "    \n",
    "    else:\n",
    "        print(f\"El archivo '{target_file}' no se encuentra en el ZIP.\")\n",
    "    df_rios_canales = df_rios_canales.rename(columns = {'Cód. CHJ' : 'id_station','Fecha' : 'date','Cantidad (hm³)' : 'quantity_hm3'})\n",
    "    df_rios_canales = df_rios_canales[['id_station', 'date','quantity_hm3']]\n",
    "    df_rios_canales['date'] = pd.to_datetime(df_rios_canales['date'], format='%d-%m-%Y %H:%M:%S')\n",
    "    df_rios_canales = df_rios_canales.dropna()\n",
    "    df_rios_canales['quantity_hm3'] = df_rios_canales['quantity_hm3'].str.replace(',','.').astype('float')\n",
    "    id_stations_list = []\n",
    "    for pixel in range(176,301):\n",
    "        id_stations = extract_infos (pixel)\n",
    "        id_stations['location_id'] = pixel\n",
    "        id_stations_list.append(id_stations)     \n",
    "    id_stations_df = pd.concat(id_stations_list)\n",
    "    df_rios_canales = df_rios_canales[df_rios_canales['date'] > fecha_inicial]\n",
    "    id_stations_df = id_stations_df[['id_station_rios_canales','location_id_rios']].drop_duplicates()\n",
    "    id_stations_df= id_stations_df.rename(columns = {'id_station_rios_canales' : 'id_station', 'location_id_rios': 'location_id'})\n",
    "    df_rios_canales = pd.merge(df_rios_canales, id_stations_df, on = 'id_station')\n",
    "    return df_rios_canales\n",
    "\n",
    "# def extraccionEmbalsesCHJ():\n",
    "#     url = \"https://aps.chj.es/down/CSV/F2797_Embalses_ROEA.zip\"\n",
    "#     fecha_inicial = pd.to_datetime(extraerUltimasFechasCopernicus())\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()  # Verificar si la descarga fue exitosa\n",
    "    \n",
    "#     # Paso 2: Cargar el contenido del ZIP en memoria\n",
    "#     zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "#     target_file = \"F2797_D1_Serie mes.xlsx\"\n",
    "#     if target_file in zip_file.namelist():\n",
    "#         with zip_file.open(target_file) as file:\n",
    "#             # Leer el archivo CSV directamente como DataFrame\n",
    "#             df_embalses = pd.read_excel(file)  # Ajusta el separador y la codificación si es necesario\n",
    "    \n",
    "#     else:\n",
    "#         print(f\"El archivo '{target_file}' no se encuentra en el ZIP.\")\n",
    "#     df_embalses = df_embalses.rename(columns = {'Cód. Embalse' : 'id_station','Fecha' : 'date','Volumen (hm³)' : 'quantity_hm3'})\n",
    "#     df_embalses = df_embalses[['id_station', 'date','quantity_hm3']]\n",
    "#     df_embalses['date'] = pd.to_datetime(df_rios_canales['date'], format='%d-%m-%Y %H:%M:%S')\n",
    "#     df_embalses = df_rios_canales.dropna()\n",
    "#     df_embalses['quantity_hm3'] = df_embalses['quantity_hm3'].str.replace(',','.').astype('float')\n",
    "#     id_stations_list = []\n",
    "#     for pixel in range(176,301):\n",
    "#         id_stations = extract_infos (pixel)\n",
    "#         id_stations['location_id'] = pixel\n",
    "#         id_stations_list.append(id_stations)     \n",
    "#     id_stations_df = pd.concat(id_stations_list)\n",
    "#     df_embalses = df_embalses[df_embalses['date'] > fecha_inicial]\n",
    "#     id_stations_df = id_stations_df[['id_station_rios_canales','location_id_embalse']].drop_duplicates()\n",
    "#     id_stations_df= id_stations_df.rename(columns = {'id_station_embalse' : 'id_station'})\n",
    "#     df_embalses = pd.merge(df_embalses, id_stations_df, on = 'id_station')\n",
    "#     return df_embalses\n",
    "\n",
    "def preparacionIngesta(df_copernicus, df_rios):\n",
    "    #tabla fechas\n",
    "    df_copernicus['date'] = pd.to_datetime(df_copernicus['date'])\n",
    "    dates1 = df_copernicus['date'].dropna().drop_duplicates().reset_index(drop = True)\n",
    "    df_date1 = pd.DataFrame({'date': dates1})\n",
    "    df_date1['date_id'] = df_date1['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    df_rios['date'] = pd.to_datetime(df_rios['date'])\n",
    "    dates2 = df_rios['date'].dropna().drop_duplicates().reset_index(drop = True)\n",
    "    df_date2 = pd.DataFrame({'date': dates2})\n",
    "    df_date2['date_id'] = df_date2['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    df_date = pd.concat([df_date1, df_date2])\n",
    "    df_date = df_date.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    #tabla df_copernicus\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    query = f'''\n",
    "        SELECT * FROM locations_id WHERE Type == 'Copernicus'\n",
    "            ;\n",
    "    '''\n",
    "    \n",
    "    df_loc = pd.read_sql_query(query, conn)\n",
    "    cursor.execute(query)\n",
    "    conn.close()\n",
    "    \n",
    "    df_copernicus = pd.merge(df_copernicus, df_loc[['latitude', 'longitude','location_id']], on = ['latitude', 'longitude'], how = 'inner')\n",
    "    \n",
    "    df_copernicus = pd.merge(df_copernicus, df_date, on = ['date'], how = 'left')\n",
    "    df_copernicus = df_copernicus.drop(['latitude', 'longitude', 'date'], axis = 1)\n",
    "    \n",
    "    #tabla ríos\n",
    "    df_rios = pd.merge(df_rios, df_date, on = 'date', how = 'left')\n",
    "    df_rios = df_rios[['quantity_hm3','location_id','date_id']]\n",
    "    return df_copernicus, df_rios, df_date\n",
    "\n",
    "def ingesta(df_copernicus, df_rios, df_date):\n",
    "    # Conexión a la base de datos\n",
    "    connection = sqlite3.connect(\"aguaCHJucar.db\")\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        df_date[\"date\"] = df_date[\"date\"].astype(str)\n",
    "        # Inserciones para df_copernicus\n",
    "        for row in df_copernicus.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_copernicus (\n",
    "                    total_precipitation,\n",
    "                    skin_temperature, evaporation, runoff, snowfall,\n",
    "                    soil_water_l1, soil_water_l2, soil_water_l3, soil_water_l4,\n",
    "                    high_vegetation_cover, low_vegetation_cover,\n",
    "                    type_high_vegetation, type_low_vegetation,location_id, date_id\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_copernicus: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Inserciones para df_rios_canales\n",
    "        for row in df_rios.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_rios_canales (\n",
    "                    quantity_hm3, location_id, date_id\n",
    "                ) VALUES (?, ?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_rios_canales: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Inserciones para df_date\n",
    "        for row in df_date.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_date (\n",
    "                    date, date_id\n",
    "                ) VALUES (?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_date: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Confirmar todas las inserciones\n",
    "        connection.commit()\n",
    "        print(\"Inserciones realizadas con éxito.\")\n",
    "    \n",
    "    except sqlite3.Error as e:\n",
    "        # Manejo de errores con rollback en caso de fallo\n",
    "        connection.rollback()\n",
    "        print(f\"Error general: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Cerrar la conexión\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef249da-564d-489f-9f27-a98234d6332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:03:40,093 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-07 15:03:40,093 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-07 15:03:40,093 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-07 15:03:40,109 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-07 15:03:40,598 WARNING [2024-10-10T00:00:00] From 1 July to 17 November 2024, the final ERA5 product is different to ERA5T due to the correction of [the assimilation of incorrect snow observations on the Alps](https://confluence.ecmwf.int/x/USuXGw)\n",
      "2025-01-07 15:03:40,598 INFO Request ID is ed1597be-8d9b-4305-b4c3-084729a59061\n",
      "2025-01-07 15:03:40,782 INFO status has been updated to accepted\n",
      "2025-01-07 15:03:49,754 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9ff3e6f1b922dfe2afb99458c5878c92.zip:   0%|          | 0.00/209k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado: 9ff3e6f1b922dfe2afb99458c5878c92.zip\n",
      "Archivo 'data_stream-enda_stepType-instant.nc' extraído.\n",
      "Archivo 'data_stream-enda_stepType-accum.nc' extraído.\n",
      "Datos de coeprnicus con fechas de 2024-09-06 a 2024-09-30 descargadas correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:04:09,071 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-07 15:04:09,072 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-07 15:04:09,074 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-07 15:04:09,076 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-07 15:04:09,733 WARNING [2024-10-10T00:00:00] From 1 July to 17 November 2024, the final ERA5 product is different to ERA5T due to the correction of [the assimilation of incorrect snow observations on the Alps](https://confluence.ecmwf.int/x/USuXGw)\n",
      "2025-01-07 15:04:09,735 INFO Request ID is ad6a55f5-73b2-4c13-81c2-912e89a01ca5\n",
      "2025-01-07 15:04:09,830 INFO status has been updated to accepted\n",
      "2025-01-07 15:04:18,439 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b29bac636f004150d95bfd65faf9463.zip:   0%|          | 0.00/223k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado: b29bac636f004150d95bfd65faf9463.zip\n",
      "Archivo 'data_stream-enda_stepType-instant.nc' extraído.\n",
      "Archivo 'data_stream-enda_stepType-accum.nc' extraído.\n",
      "Datos de coeprnicus con fechas de 2024-10-01 a 2024-10-31 descargadas correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:04:37,376 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-07 15:04:37,392 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-07 15:04:37,392 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-07 15:04:37,392 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-07 15:04:37,879 WARNING [2024-10-10T00:00:00] From 1 July to 17 November 2024, the final ERA5 product is different to ERA5T due to the correction of [the assimilation of incorrect snow observations on the Alps](https://confluence.ecmwf.int/x/USuXGw)\n",
      "2025-01-07 15:04:37,881 INFO Request ID is 09a3ef2b-1c8c-4112-875e-6b735f6bd766\n",
      "2025-01-07 15:04:37,997 INFO status has been updated to accepted\n",
      "2025-01-07 15:04:46,474 INFO status has been updated to running\n",
      "2025-01-07 15:04:51,792 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ece78589df2915facc512ed759ef2a82.zip:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado: ece78589df2915facc512ed759ef2a82.zip\n",
      "Archivo 'data_stream-enda_stepType-instant.nc' extraído.\n",
      "Archivo 'data_stream-enda_stepType-accum.nc' extraído.\n",
      "Datos de coeprnicus con fechas de 2024-11-01 a 2024-11-30 descargadas correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:05:05,161 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-07 15:05:05,161 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-07 15:05:05,176 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-07 15:05:05,176 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-07 15:05:05,614 WARNING [2024-10-10T00:00:00] From 1 July to 17 November 2024, the final ERA5 product is different to ERA5T due to the correction of [the assimilation of incorrect snow observations on the Alps](https://confluence.ecmwf.int/x/USuXGw)\n",
      "2025-01-07 15:05:05,614 INFO Request ID is da78ae9b-3385-44f5-b388-f037f1cf6f26\n",
      "2025-01-07 15:05:05,708 INFO status has been updated to accepted\n",
      "2025-01-07 15:05:14,744 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "641b27357ad35d10ff89cbebe4f52e22.zip:   0%|          | 0.00/219k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado: 641b27357ad35d10ff89cbebe4f52e22.zip\n",
      "Archivo 'data_stream-enda_stepType-instant.nc' extraído.\n",
      "Archivo 'data_stream-enda_stepType-accum.nc' extraído.\n",
      "Datos de coeprnicus con fechas de 2024-12-01 a 2024-12-31 descargadas correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:05:29,791 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2025-01-07 15:05:29,806 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-01-07 15:05:29,806 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2025-01-07 15:05:29,806 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-01-07 15:05:30,322 WARNING [2024-10-10T00:00:00] From 1 July to 17 November 2024, the final ERA5 product is different to ERA5T due to the correction of [the assimilation of incorrect snow observations on the Alps](https://confluence.ecmwf.int/x/USuXGw)\n",
      "2025-01-07 15:05:30,325 INFO Request ID is 09356e6e-d811-4b40-84fa-9cffb47830c0\n",
      "2025-01-07 15:05:30,521 INFO status has been updated to accepted\n",
      "2025-01-07 15:05:40,310 INFO status has been updated to running\n",
      "2025-01-07 15:05:49,117 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3d6f75bbf55ab03de8ad69e4af67b132.zip:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado: 3d6f75bbf55ab03de8ad69e4af67b132.zip\n",
      "Archivo 'data_stream-enda_stepType-instant.nc' extraído.\n",
      "Archivo 'data_stream-enda_stepType-accum.nc' extraído.\n",
      "Datos de coeprnicus con fechas de 2025-01-01 a 2025-01-02 descargadas correctamente\n"
     ]
    }
   ],
   "source": [
    "df_copernicus_abs = actualizarTablaCopernicus()\n",
    "df_rios_abs = extraccionRiosCHJ()\n",
    "#df_copernicus, df_rios, df_date = preparacionIngesta(df_copernicus, df_rios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0132ad8d-b8f9-4f36-a52c-4ceabdf5b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copernicus_abs.to_csv('df_copernicus_abs.csv', index = False)\n",
    "df_rios_abs.to_csv('df_rios_abs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86bcee75-95fb-452a-8fa8-5ecc9d8ffffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copernicus_abs = pd.read_csv('df_copernicus_abs.csv')\n",
    "df_rios_abs = pd.read_csv('df_rios_abs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88130773-befb-4675-b2a1-093316b2d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copernicus = df_copernicus_abs.copy()\n",
    "df_rios = df_rios_abs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36cf517-b161-4f56-87b2-c24a0e0df3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copernicus, df_rios, df_date = preparacionIngesta(df_copernicus, df_rios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1424532a-3397-42b5-b796-30a0991d5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserciones realizadas con éxito.\n"
     ]
    }
   ],
   "source": [
    "ingesta(df_copernicus, df_rios, df_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5259bec2-0e8f-4fcd-b771-fb31706d6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"aguaCHJucar.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Inspecciona la tabla df_copernicus\n",
    "query = \"\"\"SELECT \n",
    "        d.date,\n",
    "        quantity_hm3,\n",
    "        pc.location_id_copernicus AS location_id\n",
    "        FROM df_rios_canales rios \n",
    "        JOIN df_date d ON d.date_id = rios.date_id \n",
    "        JOIN df_pixeles_cercanos pc ON pc.location_id_rios_canales = rios.location_id;\n",
    "        \"\"\"\n",
    "cursor.execute(query)\n",
    "df_rios_canales = pd.read_sql_query(query, conn)\n",
    "   \n",
    "conn.close()\n",
    "df_rios_canales['date'] = pd.to_datetime(df_rios_canales['date'])\n",
    "df_rios_canales = df_rios_canales.groupby(['date', 'location_id']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae96c7a4-07bf-4798-92ba-36aa7e705782",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"aguaCHJucar.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Inspecciona la tabla df_copernicus\n",
    "query = \"\"\"SELECT \n",
    "            d.date,\n",
    "            c.total_precipitation,\n",
    "            c.skin_temperature,\n",
    "            c.evaporation,\n",
    "            c.runoff,\n",
    "            c.snowfall,\n",
    "            c.soil_water_l1,\n",
    "            c.soil_water_l2,\n",
    "            c.soil_water_l3,\n",
    "            c.soil_water_l4,\n",
    "            c.high_vegetation_cover,\n",
    "            c.low_vegetation_cover,\n",
    "            c.location_id\n",
    "            FROM df_copernicus c JOIN df_date d ON d.date_id = c.date_id;\"\"\"\n",
    "cursor.execute(query)\n",
    "df_coper = pd.read_sql_query(query, conn)\n",
    "df_coper['date'] = pd.to_datetime(df_coper['date'])\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30edeacc-1a57-4e70-810b-2232d48a84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_coper, df_rios_canales, on = ['date', 'location_id'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b01bdd8-0209-4225-8d6a-e998bfca8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soil_water'] = df['soil_water_l1'] + df['soil_water_l2'] + df['soil_water_l3'] + df['soil_water_l4']\n",
    "df = df.drop(['soil_water_l1', 'soil_water_l2', 'soil_water_l3','soil_water_l4'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deead835-204d-49b1-a247-9f64670ae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Func_analisis import calculate_df_retardos\n",
    "combined_df = []\n",
    "for pixel in df['location_id'].unique():\n",
    "    df_pixel = df[df['location_id'] == pixel]\n",
    "    df_pixel = calculate_df_retardos(df = df_pixel,retardosMax = 50)\n",
    "    combined_df.append(df_pixel)\n",
    "df_retard = pd.concat(combined_df)\n",
    "df_retard.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14be484b-5bc9-4560-894a-7c87b201bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def apply_pca(df, var, frec, var_threshold=95, imprimir = False):\n",
    "    \"\"\"\n",
    "    Aplica PCA a las columnas generadas por retardAvg_tNat y retardAgg_tNat\n",
    "    para una frecuencia específica, añadiendo el porcentaje de varianza explicada\n",
    "    hasta alcanzar el umbral especificado.\n",
    "    \n",
    "    df: DataFrame procesado.\n",
    "    var: Variable base usada en las funciones de retardos (como 'total_precipitation').\n",
    "    frec: Frecuencia temporal ('D', 'M', 'Y').\n",
    "    var_threshold: Umbral de varianza explicada acumulada (%) para decidir el número de componentes principales.\n",
    "    \n",
    "    Retorna:\n",
    "        - Un DataFrame con las componentes principales.\n",
    "    \"\"\"\n",
    "    # Filtrar las columnas relevantes para la frecuencia y la variable\n",
    "    cols_to_pca = [\n",
    "        col for col in df.columns \n",
    "        if var in col and f'{frec}' in col and ('_sum_' in col or '_mean_' in col)\n",
    "    ]\n",
    "    \n",
    "    if not cols_to_pca:\n",
    "        #print(f\"No hay columnas para aplicar PCA con frecuencia {frec} para la variable '{var}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Normalizar las columnas antes de PCA\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(df[cols_to_pca].fillna(0))  # Reemplazar NaN con 0 para evitar problemas\n",
    "    \n",
    "    # Aplicar PCA sin límite de componentes\n",
    "    pca = PCA()\n",
    "    pca_components = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calcular la varianza explicada acumulada\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_ * 100  # Convertir a porcentaje\n",
    "    cumulative_explained_variance = explained_variance_ratio.cumsum()  # Cálculo acumulado\n",
    "    \n",
    "    # Determinar el número de componentes que alcanzan el umbral de varianza explicada\n",
    "    n_components = (cumulative_explained_variance <= var_threshold).sum() + 1\n",
    "    \n",
    "    # Aplicar PCA nuevamente con el número de componentes necesario\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Crear nombres para las componentes principales, incluyendo la variable y frecuencia\n",
    "    pca_columns = [\n",
    "        f'{var}_PCA_{frec}_comp{i+1}' for i in range(n_components)\n",
    "    ]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las componentes principales\n",
    "    pca_df = pd.DataFrame(pca_components, columns=pca_columns)\n",
    "    \n",
    "    \n",
    "    # Imprimir resultados de la varianza explicada\n",
    "    if imprimir:\n",
    "        print(f\"PCA aplicado para variable '{var}' y frecuencia '{frec}'.\")\n",
    "        print(f\"Varianza explicada acumulada para los {n_components} componentes: {cumulative_explained_variance[n_components-1]}%\")\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "def process_pca_for_variables(df, var_threshold=80):\n",
    "    \"\"\"\n",
    "    Aplica PCA para cada combinación de variable y frecuencia temporal, \n",
    "    y combina los resultados en un único archivo Pickle con todas las combinaciones.\n",
    "    \n",
    "    Args:\n",
    "        df_filename (str): Nombre del archivo Pickle que contiene el DataFrame original.\n",
    "        var_threshold (float): Umbral mínimo de varianza explicada acumulada para las componentes principales.\n",
    "    \n",
    "    Returns:\n",
    "        str: Nombre del archivo Pickle con los resultados de PCA.\n",
    "        pd.Series: Fechas originales.\n",
    "        pd.Series: Pixeles originales.\n",
    "        pd.Series: Valores de 'quantity_hm3'.\n",
    "    \"\"\"\n",
    "    dates = df['date']\n",
    "    location_id = df['location_id']\n",
    "    quantity_hm3 = df['quantity_hm3']\n",
    "    # Definir las variables a procesar\n",
    "    variables = ['total_precipitation', 'skin_temperature', 'evaporation', 'runoff',\n",
    "           'snowfall', 'high_vegetation_cover', 'low_vegetation_cover',\n",
    "           'soil_water']\n",
    "    frecuencias = ['D', 'M', 'Y']\n",
    "    \n",
    "    # Inicializar el diccionario para resultados PCA\n",
    "    pca_results = []\n",
    "    \n",
    "    # Aplicar PCA para cada combinación de variable y frecuencia\n",
    "    for var in variables:\n",
    "        for frec in frecuencias:\n",
    "            pca_df = apply_pca(df, var=var, frec=frec, var_threshold=var_threshold)\n",
    "            pca_results.append(pca_df)\n",
    "    df_final = pd.concat(pca_results, axis = 1).reset_index(drop = True)\n",
    "    df_final['date'] = dates\n",
    "    df_final['location_id'] = location_id\n",
    "    df_final['quantity_hm3'] = quantity_hm3\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11faf10-54d9-4759-af03-f672372f5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = process_pca_for_variables(df = df_retard, var_threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8ac3a19-55c5-4266-9e55-78624f1aa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting(df):\n",
    "    X = df.drop(['quantity_hm3'], axis=1)  \n",
    "    y = df['quantity_hm3']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=20,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = xgb_model.predict(X_train)\n",
    "    y_test_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(train_r2)\n",
    "    print(test_r2)\n",
    "    #logging.info(f\"Modelo gradient boosting entrenado con éxito\")\n",
    "    return xgb_model\n",
    "    \n",
    "def create_df_retardos_prediccion(df,retardosMax):\n",
    "    '''\n",
    "    Para validar modelo haciendo una predicción ante todos los pixels.\n",
    "    Imprescindible columna pixel \n",
    "    df es el data frame de la extraccion incial\n",
    "    componentes pasarlo con la funcion extract_n_components(df.columns) siendo df el data frame con el PCA pasado\n",
    "    '''\n",
    "    \n",
    "    locations = df['location_id'].unique()\n",
    "    \n",
    "    df_preds_list = []\n",
    "    for loc in locations:\n",
    "        df_pixel = df[df['location_id'] == loc].copy()\n",
    "        df_pixel.drop(['location_id'],axis = 1, inplace = True)\n",
    "        fecha_inicial  = df_pixel['date'].max()\n",
    "        fecha_final = fecha_inicial+ pd.Timedelta(days=retardosMax) \n",
    "        df_date_range = pd.DataFrame(pd.date_range(start=fecha_inicial, end=fecha_final, freq='D'), columns=['date'])\n",
    "        df_pixel = pd.merge(df_date_range, df_pixel, on='date', how='outer').sort_values('date')\n",
    "        df_pixel = calculate_df_retardos(df = df_pixel,retardosMax = retardosMax)\n",
    "        df_pixel['location_id'] = loc\n",
    "        df_preds_list.append(df_pixel)       \n",
    "    df_preds = pd.concat(df_preds_list)\n",
    "    df_preds = df_preds.reset_index(drop = True)\n",
    "    #df_preds = df_preds.dropna(subset=['location_id'])\n",
    "    # logging.info(f\"Retardos de la predicción calculados con éxito\")\n",
    "    return df_preds\n",
    "\n",
    "def process_pca_custom_components(df, variables, frecuencias, n_components_dict, imprimir=False):\n",
    "    \"\"\"\n",
    "    Aplica PCA para cada combinación de variable y frecuencia temporal, con un número de componentes especificado.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame original.\n",
    "        variables (list): Lista de variables base para aplicar PCA.\n",
    "        frecuencias (list): Lista de frecuencias temporales ('D', 'M', 'Y').\n",
    "        n_components_dict (dict): Diccionario donde las claves son combinaciones 'var_frec' y los valores el número de componentes deseados.\n",
    "                                  Ejemplo: {'total_precipitation_D': 3, 'evaporation_M': 2}\n",
    "        imprimir (bool): Si True, imprime detalles sobre el PCA aplicado.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las componentes principales combinadas.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    import pandas as pd\n",
    "\n",
    "    # Crear un diccionario para almacenar los resultados de PCA\n",
    "    pca_results = []\n",
    "\n",
    "    for var in variables:\n",
    "        for frec in frecuencias:\n",
    "            # Identificar las columnas relevantes para la variable y frecuencia\n",
    "            cols_to_pca = [\n",
    "                col for col in df.columns \n",
    "                if var in col and f'{frec}' in col and ('_sum_' in col or '_mean_' in col)\n",
    "            ]\n",
    "\n",
    "            if not cols_to_pca:\n",
    "                if imprimir:\n",
    "                    print(f\"No hay columnas para PCA en '{var}_{frec}'.\")\n",
    "                continue\n",
    "\n",
    "            # Normalizar los datos\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(df[cols_to_pca].fillna(0))\n",
    "\n",
    "            # Determinar la cantidad de componentes para esta variable y frecuencia\n",
    "            key = f'{var}_{frec}'\n",
    "            n_components = n_components_dict.get(key, None)\n",
    "\n",
    "            if n_components is None:\n",
    "                if imprimir:\n",
    "                    print(f\"Cantidad de componentes no especificada para '{key}', saltando.\")\n",
    "                continue\n",
    "\n",
    "            # Aplicar PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "            # Crear nombres para las componentes principales\n",
    "            pca_columns = [f'{var}_PCA_{frec}_comp{i+1}' for i in range(n_components)]\n",
    "\n",
    "            # Crear DataFrame con las componentes principales\n",
    "            pca_df = pd.DataFrame(pca_components, columns=pca_columns)\n",
    "\n",
    "            # Agregar la clave temporal al nuevo DataFrame si existe en el original\n",
    "            if 'date' in df.columns:\n",
    "                pca_df['date'] = df['date'].values\n",
    "            elif 'date' in df.index.names:\n",
    "                pca_df['date'] = df.index.get_level_values('date')\n",
    "\n",
    "            # Guardar resultados\n",
    "            pca_results.append(pca_df)\n",
    "\n",
    "            # Imprimir información opcional\n",
    "            if imprimir:\n",
    "                explained_variance = pca.explained_variance_ratio_.sum() * 100\n",
    "                print(f\"PCA para '{key}': {n_components} componentes principales seleccionadas. Varianza explicada: {explained_variance:.2f}%.\")\n",
    "\n",
    "    # Combinar los resultados en un único DataFrame\n",
    "\n",
    "    final_pca_df = pd.concat(pca_results, axis = 1)\n",
    "\n",
    "    return final_pca_df\n",
    "\n",
    "def extract_n_components(columns):\n",
    "    \"\"\"\n",
    "    Convierte una lista de columnas con formato PCA en un diccionario\n",
    "    que contiene la cantidad máxima de componentes por variable y frecuencia.\n",
    "\n",
    "    Parameters:\n",
    "        columns (list): Lista de nombres de columnas en formato '<variable>_PCA_<frecuencia>_comp<número>'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con la cantidad máxima de componentes por variable y frecuencia.\n",
    "    \"\"\"\n",
    "    n_components_dict = {}\n",
    "\n",
    "    for col in columns:\n",
    "        parts = col.split('_PCA_')\n",
    "        variable = parts[0]\n",
    "        frequency, component = parts[1].split('_comp')\n",
    "        key = f\"{variable}_{frequency}\"\n",
    "        n_components_dict[key] = max(n_components_dict.get(key, 0), int(component))\n",
    "\n",
    "    return n_components_dict\n",
    "    \n",
    "def create_df_PCA_prediccion(df_preds,df_pca ,retardosMax):\n",
    "    columnas_pca = [i for i in df_pca.columns if i not in  ['date', 'location_id', 'quantity_hm3']]\n",
    "    componentes = extract_n_components(columnas_pca)\n",
    "    location_id = df_preds['location_id']\n",
    "    dates = df_preds['date']\n",
    "    df_preds.set_index('date', inplace = True)\n",
    "    variables = ['total_precipitation', 'skin_temperature', 'evaporation', 'runoff',\n",
    "           'snowfall', 'high_vegetation_cover', 'low_vegetation_cover',\n",
    "           'soil_water']\n",
    "    frecuencias = ['D', 'M', 'Y']\n",
    "    df_preds.drop('location_id', axis = 1, inplace = True)\n",
    "    df_pca_preds = process_pca_custom_components(df = df_preds, variables = variables, frecuencias  = frecuencias, n_components_dict = componentes, imprimir=False)\n",
    "    df_pca_preds.drop(['date'],axis = 1, inplace = True)\n",
    "    df_pca_preds['date'] = dates\n",
    "    df_pca_preds['location_id'] = location_id\n",
    "    #df_pca_preds = df_pca_preds[df_pca_preds['date'] >= fecha_inicial].set_index('date')\n",
    "    # logging.info(f\"Data frame para la predicción creado con éxito\")\n",
    "    return df_pca_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adda281f-aa0b-4b65-8310-64c683dcd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = create_df_retardos_prediccion(df = df,retardosMax = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e5f714a-e79d-4948-8b87-3b084cb53997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_preds = create_df_PCA_prediccion(df_preds = df_preds, df_pca = df_pca ,retardosMax = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b696d-c3a2-4786-a9af-16b4f9bee529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2003cfb1-e1a4-4ed2-bb88-617673577849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import os\n",
    "import zipfile\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from Func_extr import extract_infos\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import time\n",
    "from Func_analisis import calculate_df_retardos\n",
    "from tqdm import tqdm\n",
    "\n",
    "##### EXTRACCIÓN DE COPERMICUS\n",
    "\n",
    "def preprocesarDataFrame(df):\n",
    "    df = df[['valid_time', 'latitude', 'longitude', 'tp', 'skt', 'e', 'ro', 'sf',\n",
    "         'swvl1', 'swvl2', 'swvl3', 'swvl4', 'cvh', 'cvl', 'tvh', 'tvl']]\n",
    "    df = df.rename(columns={\n",
    "        'valid_time': 'date',\n",
    "        'tp': 'total_precipitation',\n",
    "        'skt': 'skin_temperature',\n",
    "        'e': 'evaporation',\n",
    "        'ro': 'runoff',\n",
    "        'sf': 'snowfall',\n",
    "        'swvl1': 'soil_water_l1',\n",
    "        'swvl2': 'soil_water_l2',\n",
    "        'swvl3': 'soil_water_l3',\n",
    "        'swvl4': 'soil_water_l4',\n",
    "        'cvh': 'high_vegetation_cover',\n",
    "        'cvl': 'low_vegetation_cover',\n",
    "        'tvh': 'type_high_vegetation',\n",
    "        'tvl': 'type_low_vegetation'\n",
    "    })\n",
    "    # Convertir la columna 'date' a formato de fecha\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    # Realizar las agregaciones\n",
    "    agg_funcs = {\n",
    "        'total_precipitation': 'sum',\n",
    "        'skin_temperature': 'mean',\n",
    "        'evaporation': 'sum',\n",
    "        'runoff': 'sum',\n",
    "        'snowfall': 'sum',\n",
    "        'soil_water_l1': 'sum',\n",
    "        'soil_water_l2': 'sum',\n",
    "        'soil_water_l3': 'sum',\n",
    "        'soil_water_l4': 'sum',\n",
    "        'high_vegetation_cover': 'mean',\n",
    "        'low_vegetation_cover': 'mean',\n",
    "        'type_high_vegetation': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "        'type_low_vegetation': lambda x: x.mode()[0] if not x.mode().empty else np.nan\n",
    "    }\n",
    "    df = df.groupby(['latitude', 'longitude', 'date']).agg(agg_funcs).reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def downloadMesCopernicus(days, year, month):\n",
    "    dataset = \"reanalysis-era5-single-levels\"\n",
    "    request = {\n",
    "        'product_type': ['ensemble_mean'],\n",
    "        'variable': [\n",
    "            'total_precipitation', 'skin_temperature', 'evaporation', \n",
    "            'runoff', 'snowfall', 'volumetric_soil_water_layer_1', \n",
    "            'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3', \n",
    "            'volumetric_soil_water_layer_4', 'high_vegetation_cover', \n",
    "            'low_vegetation_cover', 'type_of_high_vegetation', 'type_of_low_vegetation'\n",
    "        ],\n",
    "        'year': year,\n",
    "        'month': month,\n",
    "        'day': days,\n",
    "        'time': ['00:00', '06:00', '12:00', '18:00'],\n",
    "        'format': 'netcdf',\n",
    "        'area': [40.5, -2.2, 38.1, 0.5]  # Cuenca hidrográfica del Júcar +/-.\n",
    "    }\n",
    "    \n",
    "    client = cdsapi.Client(\n",
    "    url=\"https://cds.climate.copernicus.eu/api\",  # URL de producción\n",
    "    key=\"3ca60b29-89a8-4614-88a3-2b4113bfc5f8\"\n",
    ")\n",
    "    \n",
    "    try:\n",
    "        file_name = client.retrieve(dataset, request).download()\n",
    "        return file_name\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Error en la descarga: {e}\")\n",
    "        return None\n",
    "    \n",
    "def procesarZip(zip_file_name):\n",
    "    dataFrames = []\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        # Listar los archivos en el ZIP\n",
    "        archivos = zip_ref.namelist()\n",
    "        for archivo_nc in archivos: \n",
    "            # Extraer el archivo 'instant'\n",
    "            zip_ref.extract(archivo_nc)\n",
    "            print(f\"Archivo '{archivo_nc}' extraído.\")\n",
    "            ds = xr.open_dataset(archivo_nc)\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "            dataFrames.append (df)\n",
    "            ds.close()\n",
    "            os.remove(archivo_nc)\n",
    "    df1 = dataFrames[1]\n",
    "    df1 = df1.drop(['latitude', 'longitude', 'valid_time'], axis = 1)\n",
    "    df = pd.concat([dataFrames[0],df1], axis = 1)\n",
    "    df = preprocesarDataFrame(df)\n",
    "    fechaMin = df['date'].min()\n",
    "    fechaMax = df['date'].max()\n",
    "    print(f'Datos de coeprnicus con fechas de {fechaMin} a {fechaMax} descargadas correctamente')\n",
    "    os.remove(zip_file_name)\n",
    "    return df\n",
    "\n",
    "def extraccionCopernicus (days,year, month):\n",
    "    \n",
    "    zip_file_name = downloadMesCopernicus(days,year, month)\n",
    "    print(f\"Archivo descargado: {zip_file_name}\")\n",
    "    if zip_file_name is None:\n",
    "        return None\n",
    "    else:\n",
    "        df = procesarZip(zip_file_name)\n",
    "    return df\n",
    "\n",
    "def extraerUltimasFechasCopernicus():\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "        c.date_id, d.date\n",
    "        FROM df_copernicus c JOIN df_date d ON  c.date_id = d.date_id;\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    df_date = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    df_date['date'] = pd.to_datetime(df_date['date']).dt.date\n",
    "    ultima_fecha = df_date['date'].max()\n",
    "    return ultima_fecha\n",
    "    \n",
    "def extraerUltimasFechasRios():\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "        c.date_id, d.date\n",
    "        FROM df_rios_canales c JOIN df_date d ON  c.date_id = d.date_id;\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    df_date = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    df_date['date'] = pd.to_datetime(df_date['date']).dt.date\n",
    "    ultima_fecha = df_date['date'].max()\n",
    "    return ultima_fecha    \n",
    "def generar_diferencias_mes_a_mes(fecha1 ,fecha2):\n",
    "    \"\"\"\n",
    "    Genera una lista de días, meses y años entre dos fechas mes a mes,\n",
    "    considerando la fecha inicial desde su día específico y el día final.\n",
    "\n",
    "    Parámetros:\n",
    "    - fecha_actual: Fecha inicial en formato datetime.date.\n",
    "    - fecha_futura: Fecha final en formato datetime.date.\n",
    "\n",
    "    Retorno:\n",
    "    - Lista de tuplas (días, año, mes), donde:\n",
    "      - días: lista de strings con los días del mes ('15', '16', ..., '30')\n",
    "      - año: año correspondiente\n",
    "      - mes: mes correspondiente\n",
    "    \"\"\"\n",
    "    diferencias = []\n",
    "    fecha1 = fecha1  + timedelta(days=1)\n",
    "    fecha_inicio = fecha1  \n",
    "\n",
    "    while fecha_inicio <= fecha2:\n",
    "        # Obtener año y mes actuales\n",
    "        year = fecha_inicio.year\n",
    "        month = fecha_inicio.month\n",
    "\n",
    "        # Si estamos en el primer mes, iniciar desde el día específico de la fecha actual\n",
    "        if fecha_inicio == fecha1:\n",
    "            start_day = fecha_inicio.day\n",
    "        else:\n",
    "            start_day = 1\n",
    "        \n",
    "        # Para el último mes (fecha_futura), restringir al día final\n",
    "        if fecha_inicio.month == fecha2.month:\n",
    "            end_day = fecha2.day\n",
    "        else:\n",
    "            # Si no es el último mes, el mes completo\n",
    "            end_day = monthrange(year, month)[1]\n",
    "\n",
    "        # Calcular los días del mes actual, considerando start_day y end_day\n",
    "        days = [f\"{day:02d}\" for day in range(start_day, end_day + 1)]\n",
    "        \n",
    "        # Agregar a la lista de diferencias\n",
    "        diferencias.append((days, year, month))\n",
    "\n",
    "        # Avanzar al siguiente mes\n",
    "        if month == 12:\n",
    "            fecha_inicio = fecha_inicio.replace(year=year + 1, month=1, day=1)\n",
    "        else:\n",
    "            fecha_inicio = fecha_inicio.replace(month=month + 1, day=1)\n",
    "    \n",
    "    return diferencias\n",
    "\n",
    "def fechasActualizarCopernicus():\n",
    "    fecha1 = extraerUltimasFechasCopernicus()\n",
    "    fecha2 = datetime.now().date()\n",
    "    fechas_new = generar_diferencias_mes_a_mes(fecha1 ,fecha2)\n",
    "    return fechas_new\n",
    "\n",
    "def actualizarTablaCopernicus():\n",
    "    fechas_new = fechasActualizarCopernicus()\n",
    "    dataFrames = []\n",
    "    for i in fechas_new:\n",
    "        days = i[0]\n",
    "        month = i[2]\n",
    "        year = i[1]\n",
    "        df = extraccionCopernicus (days,year, month)\n",
    "        if df is None:\n",
    "            pass\n",
    "        else:\n",
    "            dataFrames.append(df)\n",
    "    df = pd.concat(dataFrames)\n",
    "    return df\n",
    "\n",
    "##### EXTRACCIÓN DE LA CUENCA DEL JUCAR\n",
    "def extraccionRiosCHJ():\n",
    "    url = \"https://aps.chj.es/down/CSV/F2796_Rios_y_Canales_ROEA.zip\"\n",
    "    fecha_inicial = pd.to_datetime(extraerUltimasFechasRios())\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Verificar si la descarga fue exitosa\n",
    "    \n",
    "    # Paso 2: Cargar el contenido del ZIP en memoria\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    target_file = \"F2796_D2_Serie día.csv\"\n",
    "    if target_file in zip_file.namelist():\n",
    "        with zip_file.open(target_file) as file:\n",
    "            # Leer el archivo CSV directamente como DataFrame\n",
    "            df_rios_canales = pd.read_csv(file, sep=\";\", encoding=\"latin1\")  # Ajusta el separador y la codificación si es necesario\n",
    "    \n",
    "    else:\n",
    "        print(f\"El archivo '{target_file}' no se encuentra en el ZIP.\")\n",
    "    df_rios_canales = df_rios_canales.rename(columns = {'Cód. CHJ' : 'id_station','Fecha' : 'date','Cantidad (hm³)' : 'quantity_hm3'})\n",
    "    df_rios_canales = df_rios_canales[['id_station', 'date','quantity_hm3']]\n",
    "    df_rios_canales['date'] = pd.to_datetime(df_rios_canales['date'], format='%d-%m-%Y %H:%M:%S')\n",
    "    df_rios_canales = df_rios_canales.dropna()\n",
    "    df_rios_canales['quantity_hm3'] = df_rios_canales['quantity_hm3'].str.replace(',','.').astype('float')\n",
    "    id_stations_list = []\n",
    "    for pixel in range(176,301):\n",
    "        id_stations = extract_infos (pixel)\n",
    "        id_stations['location_id'] = pixel\n",
    "        id_stations_list.append(id_stations)     \n",
    "    id_stations_df = pd.concat(id_stations_list)\n",
    "    df_rios_canales = df_rios_canales[df_rios_canales['date'] > fecha_inicial]\n",
    "    id_stations_df = id_stations_df[['id_station_rios_canales','location_id_rios']].drop_duplicates()\n",
    "    id_stations_df= id_stations_df.rename(columns = {'id_station_rios_canales' : 'id_station', 'location_id_rios': 'location_id'})\n",
    "    df_rios_canales = pd.merge(df_rios_canales, id_stations_df, on = 'id_station')\n",
    "    return df_rios_canales\n",
    "\n",
    "# def extraccionEmbalsesCHJ():\n",
    "#     url = \"https://aps.chj.es/down/CSV/F2797_Embalses_ROEA.zip\"\n",
    "#     fecha_inicial = pd.to_datetime(extraerUltimasFechasCopernicus())\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()  # Verificar si la descarga fue exitosa\n",
    "    \n",
    "#     # Paso 2: Cargar el contenido del ZIP en memoria\n",
    "#     zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "#     target_file = \"F2797_D1_Serie mes.xlsx\"\n",
    "#     if target_file in zip_file.namelist():\n",
    "#         with zip_file.open(target_file) as file:\n",
    "#             # Leer el archivo CSV directamente como DataFrame\n",
    "#             df_embalses = pd.read_excel(file)  # Ajusta el separador y la codificación si es necesario\n",
    "    \n",
    "#     else:\n",
    "#         print(f\"El archivo '{target_file}' no se encuentra en el ZIP.\")\n",
    "#     df_embalses = df_embalses.rename(columns = {'Cód. Embalse' : 'id_station','Fecha' : 'date','Volumen (hm³)' : 'quantity_hm3'})\n",
    "#     df_embalses = df_embalses[['id_station', 'date','quantity_hm3']]\n",
    "#     df_embalses['date'] = pd.to_datetime(df_rios_canales['date'], format='%d-%m-%Y %H:%M:%S')\n",
    "#     df_embalses = df_rios_canales.dropna()\n",
    "#     df_embalses['quantity_hm3'] = df_embalses['quantity_hm3'].str.replace(',','.').astype('float')\n",
    "#     id_stations_list = []\n",
    "#     for pixel in range(176,301):\n",
    "#         id_stations = extract_infos (pixel)\n",
    "#         id_stations['location_id'] = pixel\n",
    "#         id_stations_list.append(id_stations)     \n",
    "#     id_stations_df = pd.concat(id_stations_list)\n",
    "#     df_embalses = df_embalses[df_embalses['date'] > fecha_inicial]\n",
    "#     id_stations_df = id_stations_df[['id_station_rios_canales','location_id_embalse']].drop_duplicates()\n",
    "#     id_stations_df= id_stations_df.rename(columns = {'id_station_embalse' : 'id_station'})\n",
    "#     df_embalses = pd.merge(df_embalses, id_stations_df, on = 'id_station')\n",
    "#     return df_embalses\n",
    "\n",
    "def preparacionIngesta(df_copernicus, df_rios):\n",
    "    #tabla fechas\n",
    "    df_copernicus['date'] = pd.to_datetime(df_copernicus['date'])\n",
    "    dates1 = df_copernicus['date'].dropna().drop_duplicates().reset_index(drop = True)\n",
    "    df_date1 = pd.DataFrame({'date': dates1})\n",
    "    df_date1['date_id'] = df_date1['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    df_rios['date'] = pd.to_datetime(df_rios['date'])\n",
    "    dates2 = df_rios['date'].dropna().drop_duplicates().reset_index(drop = True)\n",
    "    df_date2 = pd.DataFrame({'date': dates2})\n",
    "    df_date2['date_id'] = df_date2['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    df_date = pd.concat([df_date1, df_date2])\n",
    "    df_date = df_date.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    #tabla df_copernicus\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    query = f'''\n",
    "        SELECT * FROM locations_id WHERE Type == 'Copernicus'\n",
    "            ;\n",
    "    '''\n",
    "    \n",
    "    df_loc = pd.read_sql_query(query, conn)\n",
    "    cursor.execute(query)\n",
    "    conn.close()\n",
    "    \n",
    "    df_copernicus = pd.merge(df_copernicus, df_loc[['latitude', 'longitude','location_id']], on = ['latitude', 'longitude'], how = 'inner')\n",
    "    \n",
    "    df_copernicus = pd.merge(df_copernicus, df_date, on = ['date'], how = 'left')\n",
    "    df_copernicus = df_copernicus.drop(['latitude', 'longitude', 'date'], axis = 1)\n",
    "    \n",
    "    #tabla ríos\n",
    "    df_rios = pd.merge(df_rios, df_date, on = 'date', how = 'left')\n",
    "    df_rios = df_rios[['quantity_hm3','location_id','date_id']]\n",
    "    return df_copernicus, df_rios, df_date\n",
    "\n",
    "def ingesta(df_copernicus, df_rios, df_date):\n",
    "    # Conexión a la base de datos\n",
    "    connection = sqlite3.connect(\"aguaCHJucar.db\")\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        df_date[\"date\"] = df_date[\"date\"].astype(str)\n",
    "        # Inserciones para df_copernicus\n",
    "        for row in df_copernicus.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_copernicus (\n",
    "                    total_precipitation,\n",
    "                    skin_temperature, evaporation, runoff, snowfall,\n",
    "                    soil_water_l1, soil_water_l2, soil_water_l3, soil_water_l4,\n",
    "                    high_vegetation_cover, low_vegetation_cover,\n",
    "                    type_high_vegetation, type_low_vegetation,location_id, date_id\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_copernicus: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Inserciones para df_rios_canales\n",
    "        for row in df_rios.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_rios_canales (\n",
    "                    quantity_hm3, location_id, date_id\n",
    "                ) VALUES (?, ?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_rios_canales: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Inserciones para df_date\n",
    "        for row in df_date.itertuples(index=False):\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO df_date (\n",
    "                    date, date_id\n",
    "                ) VALUES (?, ?)\n",
    "                \"\"\", row)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error al insertar en df_date: {e}, Fila: {row}\")\n",
    "    \n",
    "        # Confirmar todas las inserciones\n",
    "        connection.commit()\n",
    "        print(\"Inserciones realizadas con éxito.\")\n",
    "    \n",
    "    except sqlite3.Error as e:\n",
    "        # Manejo de errores con rollback en caso de fallo\n",
    "        connection.rollback()\n",
    "        print(f\"Error general: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Cerrar la conexión\n",
    "        connection.close()\n",
    "\n",
    "def actualizacionBBDD():\n",
    "    df_copernicus = actualizarTablaCopernicus()\n",
    "    df_rios = extraccionRiosCHJ()\n",
    "    df_copernicus, df_rios, df_date = preparacionIngesta(df_copernicus, df_rios)\n",
    "    ingesta(df_copernicus, df_rios, df_date)\n",
    "\n",
    "def extraer_datosBBDD():\n",
    "    logging.info(f\"Extrayendo datos de la Base de datos...\")\n",
    "    conn = sqlite3.connect(\"aguaCHJucar.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Inspecciona la tabla df_copernicus\n",
    "    query = \"\"\"SELECT \n",
    "            d.date,\n",
    "            quantity_hm3,\n",
    "            pc.location_id_copernicus AS location_id\n",
    "            FROM df_rios_canales rios \n",
    "            JOIN df_date d ON d.date_id = rios.date_id \n",
    "            JOIN df_pixeles_cercanos pc ON pc.location_id_rios_canales = rios.location_id;\n",
    "            \"\"\"\n",
    "    cursor.execute(query)\n",
    "    df_rios_canales = pd.read_sql_query(query, conn)\n",
    "    logging.info(f\"Datos de ríos extraídos\")\n",
    "    # Inspecciona la tabla df_copernicus\n",
    "    query = \"\"\"SELECT \n",
    "                d.date,\n",
    "                c.total_precipitation,\n",
    "                c.skin_temperature,\n",
    "                c.evaporation,\n",
    "                c.runoff,\n",
    "                c.snowfall,\n",
    "                c.soil_water_l1,\n",
    "                c.soil_water_l2,\n",
    "                c.soil_water_l3,\n",
    "                c.soil_water_l4,\n",
    "                c.high_vegetation_cover,\n",
    "                c.low_vegetation_cover,\n",
    "                c.location_id\n",
    "                FROM df_copernicus c JOIN df_date d ON d.date_id = c.date_id;\"\"\"\n",
    "    cursor.execute(query)\n",
    "    df_coper = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    logging.info(f\"Datos climáticos de copernicus extraídos\")\n",
    "    df_coper['date'] = pd.to_datetime(df_coper['date'])\n",
    "    df_rios_canales['date'] = pd.to_datetime(df_rios_canales['date'])\n",
    "    df_rios_canales = df_rios_canales.groupby(['date', 'location_id']).mean().reset_index()\n",
    "    df = pd.merge(df_coper, df_rios_canales, on = ['date', 'location_id'], how = 'inner')\n",
    "    df['soil_water'] = df['soil_water_l1'] + df['soil_water_l2'] + df['soil_water_l3'] + df['soil_water_l4']\n",
    "    df = df.drop(['soil_water_l1', 'soil_water_l2', 'soil_water_l3','soil_water_l4'], axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculoRetardos (df, retardosMax):\n",
    "    logging.info(f\"Calculando retardos diarios, mensuales y anuales para {retardosMax} días...\")\n",
    "    combined_df = []\n",
    "    for pixel in df['location_id'].unique():\n",
    "        df_pixel = df[df['location_id'] == pixel]\n",
    "        df_pixel = calculate_df_retardos(df = df_pixel,retardosMax = 50)\n",
    "        combined_df.append(df_pixel)\n",
    "    df_retard = pd.concat(combined_df)\n",
    "    df_retard.reset_index(drop = True, inplace = True)\n",
    "    return df_retard\n",
    "def apply_pca(df, var, frec, var_threshold=80, imprimir = False):\n",
    "    \"\"\"\n",
    "    Aplica PCA a las columnas generadas por retardAvg_tNat y retardAgg_tNat\n",
    "    para una frecuencia específica, añadiendo el porcentaje de varianza explicada\n",
    "    hasta alcanzar el umbral especificado.\n",
    "    \n",
    "    df: DataFrame procesado.\n",
    "    var: Variable base usada en las funciones de retardos (como 'total_precipitation').\n",
    "    frec: Frecuencia temporal ('D', 'M', 'Y').\n",
    "    var_threshold: Umbral de varianza explicada acumulada (%) para decidir el número de componentes principales.\n",
    "    \n",
    "    Retorna:\n",
    "        - Un DataFrame con las componentes principales.\n",
    "    \"\"\"\n",
    "    # Filtrar las columnas relevantes para la frecuencia y la variable\n",
    "    cols_to_pca = [\n",
    "        col for col in df.columns \n",
    "        if var in col and f'{frec}' in col and ('_sum_' in col or '_mean_' in col)\n",
    "    ]\n",
    "    \n",
    "    if not cols_to_pca:\n",
    "        #print(f\"No hay columnas para aplicar PCA con frecuencia {frec} para la variable '{var}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Normalizar las columnas antes de PCA\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(df[cols_to_pca].fillna(0))  # Reemplazar NaN con 0 para evitar problemas\n",
    "    \n",
    "    # Aplicar PCA sin límite de componentes\n",
    "    pca = PCA()\n",
    "    pca_components = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calcular la varianza explicada acumulada\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_ * 100  # Convertir a porcentaje\n",
    "    cumulative_explained_variance = explained_variance_ratio.cumsum()  # Cálculo acumulado\n",
    "    \n",
    "    # Determinar el número de componentes que alcanzan el umbral de varianza explicada\n",
    "    n_components = (cumulative_explained_variance <= var_threshold).sum() + 1\n",
    "    \n",
    "    # Aplicar PCA nuevamente con el número de componentes necesario\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Crear nombres para las componentes principales, incluyendo la variable y frecuencia\n",
    "    pca_columns = [\n",
    "        f'{var}_PCA_{frec}_comp{i+1}' for i in range(n_components)\n",
    "    ]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las componentes principales\n",
    "    pca_df = pd.DataFrame(pca_components, columns=pca_columns)\n",
    "    \n",
    "    \n",
    "    # Imprimir resultados de la varianza explicada\n",
    "    if imprimir:\n",
    "        print(f\"PCA aplicado para variable '{var}' y frecuencia '{frec}'.\")\n",
    "        print(f\"Varianza explicada acumulada para los {n_components} componentes: {cumulative_explained_variance[n_components-1]}%\")\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "def process_pca_for_variables(df, var_threshold=80):\n",
    "    \"\"\"\n",
    "    Aplica PCA para cada combinación de variable y frecuencia temporal, \n",
    "    y combina los resultados en un único archivo Pickle con todas las combinaciones.\n",
    "    \n",
    "    Args:\n",
    "        df_filename (str): Nombre del archivo Pickle que contiene el DataFrame original.\n",
    "        var_threshold (float): Umbral mínimo de varianza explicada acumulada para las componentes principales.\n",
    "    \n",
    "    Returns:\n",
    "        str: Nombre del archivo Pickle con los resultados de PCA.\n",
    "        pd.Series: Fechas originales.\n",
    "        pd.Series: Pixeles originales.\n",
    "        pd.Series: Valores de 'quantity_hm3'.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Elaborando análisis de componentes principales (PCA)...\")\n",
    "    dates = df['date']\n",
    "    location_id = df['location_id']\n",
    "    quantity_hm3 = df['quantity_hm3']\n",
    "    # Definir las variables a procesar\n",
    "    variables = ['total_precipitation', 'skin_temperature', 'evaporation', 'runoff',\n",
    "           'snowfall', 'high_vegetation_cover', 'low_vegetation_cover',\n",
    "           'soil_water']\n",
    "    frecuencias = ['D', 'M', 'Y']\n",
    "    \n",
    "    # Inicializar el diccionario para resultados PCA\n",
    "    pca_results = []\n",
    "    \n",
    "    # Aplicar PCA para cada combinación de variable y frecuencia\n",
    "    for var in variables:\n",
    "        for frec in frecuencias:\n",
    "            pca_df = apply_pca(df, var=var, frec=frec, var_threshold=var_threshold)\n",
    "            pca_results.append(pca_df)\n",
    "    df_final = pd.concat(pca_results, axis = 1).reset_index(drop = True)\n",
    "    df_final['date'] = dates\n",
    "    df_final['location_id'] = location_id\n",
    "    df_final['quantity_hm3'] = quantity_hm3\n",
    "    logging.info(f\"Aanálisis de componentes principales (PCA) realizado con éxito\")\n",
    "    return df_final\n",
    "    \n",
    "# def gradient_boosting(df):\n",
    "#     X = df.drop(['quantity_hm3'], axis=1)  \n",
    "#     y = df['quantity_hm3']\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "#     xgb_model = XGBRegressor(\n",
    "#         n_estimators=50,\n",
    "#         max_depth=20,\n",
    "#         learning_rate=0.05,\n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=0.8,\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predicciones\n",
    "#     y_train_pred = xgb_model.predict(X_train)\n",
    "#     y_test_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "#     train_r2 = r2_score(y_train, y_train_pred)\n",
    "#     test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "#     print(train_r2)\n",
    "#     print(test_r2)\n",
    "#     #logging.info(f\"Modelo gradient boosting entrenado con éxito\")\n",
    "#     return xgb_model\n",
    "    \n",
    "def randomForest(df):\n",
    "    logging.info(f\"Se inicia el entrenamiento del modelo de random forest\")\n",
    "    X = df.drop(['quantity_hm3'], axis=1)  # Excluir 'date' si no se usa explícitamente\n",
    "    y = df['quantity_hm3']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Modelo de Random Forest con warm_start=True para entrenamiento incremental\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=1,  # Inicializamos con 1 árbol\n",
    "        max_depth=30,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        bootstrap=True,\n",
    "        warm_start=True  # Permite agregar más árboles de forma incremental\n",
    "    )\n",
    "    \n",
    "    # Entrenar árbol por árbol y mostrar progreso\n",
    "    n_estimators = 100  # Cantidad de árboles deseada\n",
    "    for i in tqdm(range(1, n_estimators + 1), desc=\"Entrenando Random Forest\"):\n",
    "        rf.set_params(n_estimators=i)  # Incrementamos el número de árboles\n",
    "        rf.fit(X_train, y_train)  # Entrenamos con más árboles\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    logging.info(f\"Modelo random forest entrenado con éxito a un R^2 {train_r2} en train y un {test_r2} en test\")\n",
    "    \n",
    "    return rf\n",
    "    \n",
    "def create_df_retardos_prediccion(df,retardosMax):\n",
    "    '''\n",
    "    Para validar modelo haciendo una predicción ante todos los pixels.\n",
    "    Imprescindible columna pixel \n",
    "    df es el data frame de la extraccion incial\n",
    "    componentes pasarlo con la funcion extract_n_components(df.columns) siendo df el data frame con el PCA pasado\n",
    "    '''\n",
    "    \n",
    "    locations = df['location_id'].unique()\n",
    "    \n",
    "    df_preds_list = []\n",
    "    for loc in locations:\n",
    "        df_pixel = df[df['location_id'] == loc].copy()\n",
    "        df_pixel.drop(['location_id'],axis = 1, inplace = True)\n",
    "        fecha_inicial  = df_pixel['date'].max()\n",
    "        fecha_final = fecha_inicial+ pd.Timedelta(days=retardosMax) \n",
    "        df_date_range = pd.DataFrame(pd.date_range(start=fecha_inicial, end=fecha_final, freq='D'), columns=['date'])\n",
    "        df_pixel = pd.merge(df_date_range, df_pixel, on='date', how='outer').sort_values('date')\n",
    "        df_pixel = calculate_df_retardos(df = df_pixel,retardosMax = retardosMax)\n",
    "        df_pixel['location_id'] = loc\n",
    "        df_preds_list.append(df_pixel)       \n",
    "    df_preds = pd.concat(df_preds_list)\n",
    "    df_preds = df_preds.reset_index(drop = True)\n",
    "    #df_preds = df_preds.dropna(subset=['location_id'])\n",
    "    # logging.info(f\"Retardos de la predicción calculados con éxito\")\n",
    "    return df_preds\n",
    "\n",
    "\n",
    "def process_pca_custom_components (df, variables, frecuencias, n_components_dict, imprimir=False):\n",
    "    \"\"\"\n",
    "    Aplica PCA para cada combinación de variable y frecuencia temporal, con un número de componentes especificado.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame original.\n",
    "        variables (list): Lista de variables base para aplicar PCA.\n",
    "        frecuencias (list): Lista de frecuencias temporales ('D', 'M', 'Y').\n",
    "        n_components_dict (dict): Diccionario donde las claves son combinaciones 'var_frec' y los valores el número de componentes deseados.\n",
    "                                  Ejemplo: {'total_precipitation_D': 3, 'evaporation_M': 2}\n",
    "        imprimir (bool): Si True, imprime detalles sobre el PCA aplicado.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las componentes principales combinadas.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Se procesa el PCA en base al entrenamiento...\")\n",
    "    # Crear un diccionario para almacenar los resultados de PCA\n",
    "    pca_results = []\n",
    "\n",
    "    for var in variables:\n",
    "        for frec in frecuencias:\n",
    "            # Identificar las columnas relevantes para la variable y frecuencia\n",
    "            cols_to_pca = [\n",
    "                col for col in df.columns \n",
    "                if var in col and f'{frec}' in col and ('_sum_' in col or '_mean_' in col)\n",
    "            ]\n",
    "\n",
    "            if not cols_to_pca:\n",
    "                if imprimir:\n",
    "                    print(f\"No hay columnas para PCA en '{var}_{frec}'.\")\n",
    "                continue\n",
    "\n",
    "            # Normalizar los datos\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(df[cols_to_pca].fillna(0))\n",
    "\n",
    "            # Determinar la cantidad de componentes para esta variable y frecuencia\n",
    "            key = f'{var}_{frec}'\n",
    "            n_components = n_components_dict.get(key, None)\n",
    "\n",
    "            if n_components is None:\n",
    "                if imprimir:\n",
    "                    print(f\"Cantidad de componentes no especificada para '{key}', saltando.\")\n",
    "                continue\n",
    "\n",
    "            # Aplicar PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "            # Crear nombres para las componentes principales\n",
    "            pca_columns = [f'{var}_PCA_{frec}_comp{i+1}' for i in range(n_components)]\n",
    "\n",
    "            # Crear DataFrame con las componentes principales\n",
    "            pca_df = pd.DataFrame(pca_components, columns=pca_columns)\n",
    "\n",
    "            # Agregar la clave temporal al nuevo DataFrame si existe en el original\n",
    "            if 'date' in df.columns:\n",
    "                pca_df['date'] = df['date'].values\n",
    "            elif 'date' in df.index.names:\n",
    "                pca_df['date'] = df.index.get_level_values('date')\n",
    "\n",
    "            # Guardar resultados\n",
    "            pca_results.append(pca_df)\n",
    "\n",
    "            # Imprimir información opcional\n",
    "            if imprimir:\n",
    "                explained_variance = pca.explained_variance_ratio_.sum() * 100\n",
    "                print(f\"PCA para '{key}': {n_components} componentes principales seleccionadas. Varianza explicada: {explained_variance:.2f}%.\")\n",
    "\n",
    "    # Combinar los resultados en un único DataFrame\n",
    "\n",
    "    final_pca_df = pd.concat(pca_results, axis = 1)\n",
    "\n",
    "    return final_pca_df\n",
    "\n",
    "def extract_n_components(columns):\n",
    "    \"\"\"\n",
    "    Convierte una lista de columnas con formato PCA en un diccionario\n",
    "    que contiene la cantidad máxima de componentes por variable y frecuencia.\n",
    "\n",
    "    Parameters:\n",
    "        columns (list): Lista de nombres de columnas en formato '<variable>_PCA_<frecuencia>_comp<número>'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con la cantidad máxima de componentes por variable y frecuencia.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Se extraen los componentes principales usados en el entrenamiento\")\n",
    "    n_components_dict = {}\n",
    "\n",
    "    for col in columns:\n",
    "        parts = col.split('_PCA_')\n",
    "        variable = parts[0]\n",
    "        frequency, component = parts[1].split('_comp')\n",
    "        key = f\"{variable}_{frequency}\"\n",
    "        n_components_dict[key] = max(n_components_dict.get(key, 0), int(component))\n",
    "\n",
    "    return n_components_dict\n",
    "    \n",
    "def create_df_PCA_prediccion(fecha_inicial,df_preds,df_pca ,retardosMax):\n",
    "    logging.info(f\"Análisis de componentes principales basados en los componentes usados en el entrenamiento\")\n",
    "    columnas_pca = [i for i in df_pca.columns if i not in  ['date', 'location_id', 'quantity_hm3']]\n",
    "    componentes = extract_n_components(columnas_pca)\n",
    "    location_id = df_preds['location_id']\n",
    "    dates = df_preds['date']\n",
    "    df_preds.set_index('date', inplace = True)\n",
    "    variables = ['total_precipitation', 'skin_temperature', 'evaporation', 'runoff',\n",
    "           'snowfall', 'high_vegetation_cover', 'low_vegetation_cover',\n",
    "           'soil_water']\n",
    "    frecuencias = ['D', 'M', 'Y']\n",
    "    df_preds.drop('location_id', axis = 1, inplace = True)\n",
    "    df_pca_preds = process_pca_custom_components(df = df_preds, variables = variables, frecuencias  = frecuencias, n_components_dict = componentes, imprimir=False)\n",
    "    df_pca_preds.drop(['date'],axis = 1, inplace = True)\n",
    "    df_pca_preds['date'] = dates\n",
    "    df_pca_preds['location_id'] = location_id\n",
    "    df_pca_preds = df_pca_preds[df_pca_preds['date'] >= fecha_inicial].set_index('date')\n",
    "    logging.info(f\"Data frame para la predicción creado con éxito\")\n",
    "    return df_pca_preds\n",
    "\n",
    "def crearDataFrameTrain(retardosMax):\n",
    "    df = extraer_datosBBDD()\n",
    "    df_retard = calculoRetardos (df = df, retardosMax = retardosMax)\n",
    "    df_pca = process_pca_for_variables(df = df_retard, var_threshold=80)\n",
    "    return df, df_pca\n",
    "\n",
    "def crearDataFramePred(df,df_pca, retardosMax):\n",
    "    fecha_inicial = df['date'].max()\n",
    "    df_preds = create_df_retardos_prediccion(df = df,retardosMax = retardosMax)\n",
    "    df_pca_preds = create_df_PCA_prediccion(fecha_inicial = fecha_inicial,df_preds = df_preds, df_pca = df_pca ,retardosMax = retardosMax)\n",
    "    return df_pca_preds\n",
    "\n",
    "def comprobarActualizarBBDD(retardosMax):\n",
    "    fecha_actual = datetime.now().date()\n",
    "    ultimaFechaCoper = extraerUltimasFechasCopernicus()\n",
    "    ultimaFechaRios = extraerUltimasFechasRios()\n",
    "    if (fecha_actual - min(ultimaFechaCoper,ultimaFechaRios)).days > retardosMax:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_Loc (pixel):\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    query = f'''\n",
    "        SELECT \n",
    "            loc_copernicus.latitude AS latitude_copernicus,\n",
    "            loc_copernicus.longitude AS longitude_copernicus,\n",
    "            loc_embalses.latitude AS latitude_embalses,\n",
    "            loc_embalses.longitude AS longitude_embalses,\n",
    "            loc_aemet.latitude AS latitude_aemet,\n",
    "            loc_aemet.longitude AS longitude_aemet,\n",
    "            loc_rios_canales.latitude AS latitude_rios_canales,\n",
    "            loc_rios_canales.longitude AS longitude_rios_canales,\n",
    "            -- Información adicional\n",
    "            -- Usar COALESCE para manejar los valores NULL\n",
    "            COALESCE(embalses_info.Embalse, 'No hay embalse') AS embalse,\n",
    "            COALESCE(embalses_info.location_id, 'No hay embalse') AS location_id_embalse,\n",
    "            COALESCE(rios_canales_info.EstacióndeAforo, 'No hay rio') AS estacion_aforo_rios_canales,\n",
    "            COALESCE(rios_canales_info.location_id, 'No hay rio') AS location_id_rios,\n",
    "            COALESCE(aemet_info.nombre, 'No hay aemet') AS nombre_aemet,\n",
    "            COALESCE(aemet_info.location_id, 'No hay aemet') AS location_id_aemet\n",
    "        FROM df_pixeles_cercanos p\n",
    "        -- Relacionar las localizaciones de la tabla df_pixeles_cercanos con las latitudes y longitudes\n",
    "        INNER JOIN locations_id loc_copernicus ON loc_copernicus.location_id = p.location_id_copernicus\n",
    "        LEFT JOIN locations_id loc_embalses ON loc_embalses.location_id = p.location_id_embalses\n",
    "        INNER JOIN locations_id loc_aemet ON loc_aemet.location_id = p.location_id_aemet\n",
    "        INNER JOIN locations_id loc_rios_canales ON loc_rios_canales.location_id = p.location_id_rios_canales\n",
    "        -- Relacionar con tablas de información adicional\n",
    "        INNER JOIN df_aemet_info aemet_info ON aemet_info.location_id = loc_aemet.location_id\n",
    "        INNER JOIN df_rios_canales_info rios_canales_info ON rios_canales_info.location_id = loc_rios_canales.location_id\n",
    "        LEFT JOIN df_embalses_info embalses_info ON embalses_info.location_id = loc_embalses.location_id\n",
    "        -- Filtrar por location_id_copernicus = {pixel}\n",
    "        WHERE p.location_id_copernicus = {pixel};\n",
    "    '''\n",
    "\n",
    "    df_loc = pd.read_sql_query(query, conn)\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    conn.close()\n",
    "    return df_loc\n",
    "\n",
    "\n",
    "def crear_tablaInfos(prediccion):\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    query = f'''\n",
    "        SELECT\n",
    "            pc.location_id_copernicus, pc.location_id_rios_canales, info.*\n",
    "            FROM df_pixeles_cercanos pc\n",
    "            JOIN locations_id l on pc.location_id_rios_canales = l.location_id\n",
    "            JOIN df_rios_canales_info info on info.location_id = l.location_id ;\n",
    "    '''\n",
    "    \n",
    "    df_infoRios = pd.read_sql_query(query, conn)\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    conn.close()\n",
    "    \n",
    "    df_infoRios = df_infoRios[['location_id_copernicus', 'Municipio','SistemadeExplotación']].drop_duplicates().reset_index(drop  =True).rename(columns = {'location_id_copernicus' : 'Pixel'})\n",
    "    \n",
    "    info_sistemasExplot = df_infoRios.groupby(\"Pixel\").agg({\n",
    "        \"Municipio\": lambda x: ', '.join(x),\n",
    "        \"SistemadeExplotación\": lambda x: ', '.join(sorted(set(val for val in x if val is not None)))  # Eliminar duplicados y ordenar\n",
    "    }).reset_index()\n",
    "    ########################\n",
    "    df_loc_list = []\n",
    "    for pixel in prediccion['location_id'].unique():\n",
    "        df_loc_i = extract_Loc(pixel)\n",
    "        df_loc_i['Pixel'] = pixel\n",
    "        df_loc_i = df_loc_i.rename(columns = {'estacion_aforo_rios_canales' : 'Río','latitude_copernicus':'Latitud', 'longitude_copernicus' : 'Longitud'})\n",
    "        df_loc_list.append(df_loc_i)\n",
    "    df_locs = pd.concat(df_loc_list)\n",
    "    \n",
    "    df_rios = df_locs[['Latitud','Longitud','Pixel', 'Río']].drop_duplicates()\n",
    "    result_rios = df_rios.groupby(['Pixel', 'Latitud', 'Longitud'])['Río'] \\\n",
    "               .apply(lambda x: ', '.join(x)).reset_index()\n",
    "    \n",
    "    # Renombrar la columna de salida para mayor claridad\n",
    "    result_rios.rename(columns={'Río': 'Ríos'}, inplace=True)\n",
    "    df_embalses = df_locs[['Pixel','embalse']]\n",
    "    result_embalses = df_embalses[df_embalses['embalse'] != \"No hay embalse\"] \\\n",
    "        .groupby('Pixel')['embalse'] \\\n",
    "        .apply(lambda x: ', '.join(x)).reset_index()\n",
    "    \n",
    "    # Renombrar la columna para mayor claridad\n",
    "    result_embalses.rename(columns={'embalse': 'Embalses'}, inplace=True)\n",
    "    \n",
    "    df_rios = df_locs[['Pixel', 'Río']].drop_duplicates()\n",
    "    contar_rios = df_rios.groupby('Pixel').size().reset_index(name='Cantidad de ríos')\n",
    "    \n",
    "    # Agregar píxeles con 0 embalses\n",
    "    all_pixels = df_rios['Pixel'].unique()\n",
    "    contar_rios = contar_rios.set_index('Pixel').reindex(all_pixels, fill_value=0).reset_index()\n",
    "    \n",
    "    df_embalses = df_locs[['Pixel', 'embalse']].drop_duplicates()\n",
    "    contar_embalses = df_embalses[df_embalses['embalse'] != \"No hay embalse\"].groupby('Pixel').size().reset_index(name='Cantidad de emblases')\n",
    "    \n",
    "    # Agregar píxeles con 0 embalses\n",
    "    all_pixels = df_embalses['Pixel'].unique()\n",
    "    \n",
    "    df_pixelesInfo = pd.merge(result_rios,contar_rios, on ='Pixel', how = 'left')\n",
    "    df_pixelesInfo = pd.merge(df_pixelesInfo, result_embalses, on = 'Pixel', how = 'left')\n",
    "    df_pixelesInfo = pd.merge(df_pixelesInfo, contar_embalses, on = 'Pixel', how = 'left')\n",
    "    df_pixelesInfo = pd.merge(df_pixelesInfo, info_sistemasExplot, on = 'Pixel', how = 'left').fillna(0).rename(columns = {'Municipio' : 'Municipio cercano'})\n",
    "    return df_pixelesInfo\n",
    "\n",
    "def descargarHistoricoRios()\n",
    "    conn = sqlite3.connect('aguaCHJucar.db')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    ids = tuple(prediccion['location_id'].unique())\n",
    "    query = f'''\n",
    "        SELECT \n",
    "        d.date,\n",
    "        pc.location_id_copernicus AS location_id,\n",
    "        rios.quantity_hm3\n",
    "        FROM df_rios_canales rios \n",
    "        JOIN df_date d ON d.date_id = rios.date_id \n",
    "        JOIN df_pixeles_cercanos pc ON pc.location_id_rios_canales = rios.location_id\n",
    "        WHERE pc.location_id_copernicus IN {ids};\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(query)\n",
    "    df_rios = pd.read_sql_query(query, conn)\n",
    "    df_rios['date'] = pd.to_datetime(df_rios['date'])\n",
    "    df_rios = df_rios.sort_values(['date','location_id']).reset_index(drop = True)\n",
    "    return df_rios\n",
    "\n",
    "\n",
    "def flujoTrabajo(retardosMax):\n",
    "    tiempo_inicial = time.time()\n",
    "    logging.info(f\"Se procede a calcular la predicción para {retardosMax} días. \\n Para información adicional sobre cómo se realiza ver: https://github.com/delatorre96/Water-Prediction\")\n",
    "    if comprobarActualizarBBDD(retardosMax = retardosMax):\n",
    "        logging.info(f\"\\nFASE 0: Actualización de la base de datos\\nComo los días a predecir son mayores que la diferencia entre lafecha actual y la última fecha de la base dedatos, se procede a actualizar la base de datos.\")\n",
    "        actualizacionBBDD()\n",
    "    else:\n",
    "        pass\n",
    "    logging.info(f\"\\nFASE 1: Extracción y preprocesado de los datos existentes en la actual base de datos\")\n",
    "    df, df_pca = crearDataFrameTrain(retardosMax = retardosMax)\n",
    "    df_pca.set_index(['date', 'location_id'], inplace = True)\n",
    "    logging.info(f\"\\nFASE 2: Entrenamiento del modelo de predicción\")\n",
    "    rtf = randomForest (df = df_pca)\n",
    "    # xgb_model = gradient_boosting(df = df_pca)\n",
    "    logging.info(f\"\\nFASE 3: Cálculo de la predicción\")\n",
    "    df_pca_preds = crearDataFramePred(df = df,df_pca = df_pca, retardosMax = retardosMax)\n",
    "    df_pca_preds.reset_index(inplace = True)\n",
    "    df_pca_preds.set_index(['date', 'location_id'], inplace = True)\n",
    "    prediccion = rtf.predict(df_pca_preds)\n",
    "    tiempo_final = time.time()\n",
    "    duracion = (tiempo_final - tiempo_inicial)/60\n",
    "    logging.info(f\"Predicción para {retardosMax} días realizada em {duracion} minutos\")\n",
    "    logging.info(f\"Se procede a escribir el excel de predicción...\")\n",
    "    df_prediccionFinal = df_pca_preds.reset_index()[['date', 'location_id']]\n",
    "    df_prediccionFinal['prediccion'] = prediccion\n",
    "    prediccion = prediccion.rename(columns = {'prediccion':'quantity_hm3'}).sort_values(['location_id','date']).reset_index(drop = True)\n",
    "    df_pixelesInfo = crear_tablaInfos(prediccion)\n",
    "    dataframes = {\n",
    "    'Predicción': prediccion,\n",
    "    'Info': df_pixelesInfo}\n",
    "    output_path = \"Resultado.xlsx\"\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        for sheet_name, df in dataframes.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    logging.info(f\"Excel escrito correctamente como {output_path}. /nEl contenido de este excel\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caae4d58-a7b5-4af1-9a4c-f31dd0536453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Se procede a calcular la predicción para 50 días. \n",
      " Para información adicional sobre cómo se realiza ver: https://github.com/delatorre96/Water-Prediction\n",
      "INFO:root:\n",
      "FASE 1: Extracción y preprocesado de los datos existentes en la actual base de datos\n",
      "INFO:root:Extrayendo datos de la Base de datos...\n",
      "INFO:root:Datos de ríos extraídos\n",
      "INFO:root:Datos climáticos de copernicus extraídos\n",
      "INFO:root:Calculando retardos diarios, mensuales y anuales para 50 días...\n",
      "INFO:root:Elaborando análisis de componentes principales (PCA)...\n",
      "INFO:root:Aanálisis de componentes principales (PCA) realizado con éxito\n",
      "INFO:root:\n",
      "FASE 2: Entrenamiento del modelo de predicción\n",
      "INFO:root:Se inicia el entrenamiento del modelo de random forest\n",
      "Entrenando Random Forest: 100%|████████████████████████████████████████████████████| 100/100 [2:24:58<00:00, 86.99s/it]\n",
      "INFO:root:Modelo random forest entrenado con éxito a un R^2 0.8580779450040332 en train y un 0.8289459574607626 en test\n",
      "INFO:root:\n",
      "FASE 3: Cálculo de la predicción\n",
      "INFO:root:Análisis de componentes principales basados en los componentes usados en el entrenamiento\n",
      "INFO:root:Se extraen los componentes principales usados en el entrenamiento\n",
      "INFO:root:Se procesa el PCA en base al entrenamiento...\n",
      "INFO:root:Data frame para la predicción creado con éxito\n",
      "INFO:root:Predicción para 50 días realizada em 151.90366695721943 minutos\n"
     ]
    }
   ],
   "source": [
    "prediccion = flujoTrabajo(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5144fb5-9c90-4078-b86a-d02e4bfe2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion.to_csv('prediccion_13_01_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0927da35-9ff6-400f-a305-5137bb046a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a15d85c0-7503-4b27-9de7-d34275c10e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'Predicción': prediccion,\n",
    "    #'Histórico': df_rios,\n",
    "    'Info': df_pixelesInfo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3a1e250e-25e6-4119-8f97-02793f24bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"Resultado.xlsx\"\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in dataframes.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a3ffc-6d56-4a48-88d4-0c3a92d04ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
